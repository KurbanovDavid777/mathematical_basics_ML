# Laboratory Work 1 ‚Äî Matrix Decomposition Algorithms. PCA
# –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 1 ‚Äî –ê–ª–≥–æ—Ä–∏—Ç–º—ã —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü. PCA

---

## üá∑üá∫ –û–ø–∏—Å–∞–Ω–∏–µ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã

### –ù–∞–∑–≤–∞–Ω–∏–µ
**–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 1: –ê–ª–≥–æ—Ä–∏—Ç–º—ã —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü. –ú–µ—Ç–æ–¥ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (PCA)**

### –¶–µ–ª—å —Ä–∞–±–æ—Ç—ã
–ò–∑—É—á–∏—Ç—å –º–µ—Ç–æ–¥—ã –º–∞—Ç—Ä–∏—á–Ω–æ–≥–æ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è –∏ –∏—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ –∞–Ω–∞–ª–∏–∑–µ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è –º–µ—Ç–æ–¥ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (PCA) –∏ Kernel PCA –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

### –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
–í —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –º–∞—Ç—Ä–∏—á–Ω–æ–≥–æ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö. –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ ‚Äî PCA, –∫–æ—Ç–æ—Ä—ã–π —Å–Ω–∏–∂–∞–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å, –ø—Ä–æ–µ—Ü–∏—Ä—É—è –¥–∞–Ω–Ω—ã–µ –≤ –Ω–æ–≤–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, —Å–æ—Ö—Ä–∞–Ω—è—é—â–µ–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–∏—Å–ø–µ—Ä—Å–∏—é.

–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å **Kernel PCA** –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ª–∏–Ω–µ–π–Ω–æ-–Ω–µ—Ä–∞–∑–¥–µ–ª–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

---

##  –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å

–ü—É—Å—Ç—å –¥–∞–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:

\[
X = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}, \quad x_i \in \mathbb{R}^m
\]

–î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å **—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –∫–∞–∂–¥–æ–º—É –ø—Ä–∏–∑–Ω–∞–∫—É**.

–õ–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:

\[
t_i = x_i W
\]

–≥–¥–µ:
- \(W \in \mathbb{R}^{m \times s}\) ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç,
- \(s\) ‚Äî —á–∏—Å–ª–æ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç.

–ú–∞—Ç—Ä–∏—Ü–∞ \(W\) —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –∏–∑ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –º–∞—Ç—Ä–∏—Ü—ã –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏ \(X^T X\). –í–∫–ª–∞–¥ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª–µ–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–º—É —Å–∏–Ω–≥—É–ª—è—Ä–Ω–æ–º—É —á–∏—Å–ª—É.

---

##  –°–∏–Ω–≥—É–ª—è—Ä–Ω–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ

\[
X = U \Sigma V^T
\]

–≥–¥–µ:
- \(U\) ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ –ª–µ–≤—ã—Ö —Å–∏–Ω–≥—É–ª—è—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤,
- \(\Sigma\) ‚Äî –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Å–∏–Ω–≥—É–ª—è—Ä–Ω—ã—Ö —á–∏—Å–µ–ª,
- \(V\) ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∞–≤—ã—Ö —Å–∏–Ω–≥—É–ª—è—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤.

–°–∏–Ω–≥—É–ª—è—Ä–Ω—ã–µ —á–∏—Å–ª–∞ —á–µ—Ä–µ–∑ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:

\[
\sigma_i = \sqrt{\lambda_i(XX^T)}
\]

---

##  QR-–∞–ª–≥–æ—Ä–∏—Ç–º

–î–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è **QR-–∞–ª–≥–æ—Ä–∏—Ç–º**:

\[
A = QR
\]

QR-—Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ —á–µ—Ä–µ–∑ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –ì—Ä–∞–º–∞‚Äì–®–º–∏–¥—Ç–∞.

---

##  –ó–∞–¥–∞–Ω–∏—è

### 1. –ú–µ—Ç–æ–¥ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (PCA)

1. –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–∏–Ω–≥—É–ª—è—Ä–Ω–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ **–±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è**:
   - `numpy.linalg.svd`
   - `numpy.linalg.eig`

   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **QR-–∞–ª–≥–æ—Ä–∏—Ç–º**.

2. –í—ã–±—Ä–∞—Ç—å ML-–∑–∞–¥–∞—á—É –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ:
   - –Ω–µ –º–µ–Ω–µ–µ **5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**;
   - –º–æ–¥–µ–ª—å: **–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å**.

3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SVD –¥–ª—è:
   - –ø–æ–ª—É—á–µ–Ω–∏—è –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç;
   - –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ–±—ä—è—Å–Ω—è–µ–º–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏.

4. –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —á–∏—Å–ª–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç:
   - –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ;
   - –ø–æ–∫–∞–∑–∞—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –≤ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ;
   - –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–º—ã—Å–ª –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ).

5. –û—Ü–µ–Ω–∏—Ç—å:
   - –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏;
   - –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ PCA.

6. *(–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ)* –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —É—Å–∫–æ—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã SVD.

---

### 2. Kernel PCA

1. –í—ã–±—Ä–∞—Ç—å –ª–∏–Ω–µ–π–Ω–æ-–Ω–µ—Ä–∞–∑–¥–µ–ª–∏–º—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö.
2. –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —è–¥—Ä–æ–≤—ã–µ –º–∞—Ç—Ä–∏—Ü—ã:
   - –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–µ —è–¥—Ä–æ;
   - RBF-—è–¥—Ä–æ;
   - —Å–∏–≥–º–æ–∏–¥–∞–ª—å–Ω–æ–µ —è–¥—Ä–æ.
3. –ü—Ä–æ–≤–µ—Å—Ç–∏ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ —è–¥—Ä–æ–≤–æ–π –º–∞—Ç—Ä–∏—Ü—ã.
4. –°—Ä–∞–≤–Ω–∏—Ç—å PCA –∏ Kernel PCA:
   - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ü–∏–π;
   - –ø—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç–∏.
5. –û—Ü–µ–Ω–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:
   - –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö;
   - –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ Kernel PCA.

---

## üá¨üáß Task Description (English Version)

### Title
**Laboratory Work 1: Matrix Decomposition Algorithms. Principal Component Analysis (PCA)**

### Objective
Study matrix decomposition techniques and their application to data analysis, including PCA and Kernel PCA for dimensionality reduction and classification.

### Description
This lab focuses on applying matrix decomposition methods to analyze data. The core method is PCA, which reduces dimensionality by projecting data into a new coordinate system that preserves maximum variance.

An additional task involves implementing **Kernel PCA** for non-linearly separable datasets.

---

##  Tasks

### 1. Principal Component Analysis (PCA)

1. Implement Singular Value Decomposition **from scratch**, without using:
   - `numpy.linalg.svd`
   - `numpy.linalg.eig`

   The **QR algorithm** is recommended.

2. Select a labeled ML dataset:
   - at least **5 features**;
   - model: **artificial neural network**.

3. Use the implemented SVD to:
   - compute principal components;
   - calculate explained variance.

4. Determine an appropriate number of components:
   - visualize transformed data;
   - analyze target distribution in component space;
   - interpret components if possible.

5. Evaluate:
   - model performance changes;
   - training time differences after PCA.

6. *(Optional)* Implement computationally efficient SVD variants.

---

### 2. Kernel PCA

1. Select a non-linearly separable dataset.
2. Implement kernel matrices:
   - polynomial kernel;
   - RBF kernel;
   - sigmoid kernel.
3. Perform spectral decomposition of the kernel matrix.
4. Compare PCA and Kernel PCA:
   - projection visualization;
   - linear separability analysis.
5. Evaluate classification metrics:
   - on original data;
   - on Kernel PCA-transformed data.

---

##  Expected Deliverables

- Jupyter Notebook with implementation.
- Visualizations and explanations.
- Conclusions on PCA and Kernel PCA effectiveness.

